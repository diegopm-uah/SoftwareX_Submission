{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "ISAR\n",
      "Classification\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import  matplotlib.pyplot   as plt\n",
    "import math\n",
    "import logging\n",
    "# from torchsummary import summary\n",
    "# from pytorch3d.loss import chamfer_distance\n",
    "import csv\n",
    "import time\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "\n",
    "torch.manual_seed(101)\n",
    "torch.cuda.manual_seed(101)\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\") \n",
    "\n",
    "class args():\n",
    "    def __init__(self, data_type, use_case):\n",
    "        self.data_type = data_type\n",
    "        self.use_case = use_case\n",
    "\n",
    "args = args(\"ISAR\", \"Classification\")\n",
    "print(args.data_type)\n",
    "print(args.use_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"Custom dataset for loading data files and their corresponding labels.\n",
    "\n",
    "    Initializes the CustomDataset instance.\n",
    "\n",
    "    Args:\n",
    "        npy_dir (str): Directory containing the data files.\n",
    "        labels_file (str): File path to the file containing labels.\n",
    "        transform_ISAR (callable, optional): Optional transform to be applied on a sample for ISAR data type.\n",
    "        transform_npy (callable, optional): Optional transform to be applied on a sample for npy data type.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dir, transform_ISAR=None, transform_npy=None):\n",
    "\n",
    "        self.dir = dir\n",
    "\n",
    "        if args.data_type == 'npy':\n",
    "            # List all .npy files in the directory.\n",
    "            self.file_names = [f for f in os.listdir(dir) if f.endswith('.npy') and f.startswith('sample_')]\n",
    "            self.transform = transform_npy\n",
    "        elif args.data_type == 'ISAR':\n",
    "            # List all .npy files in the directory.\n",
    "            self.file_names = [f for f in os.listdir(dir) if f.endswith('.png') and f.startswith('sample_')]\n",
    "            self.transform = transform_ISAR\n",
    "            \n",
    "        if args.use_case == \"Classification\":\n",
    "            _, _, result = dir.split('/')[-1].partition('_')  # divide en 3 partes: antes, separador, después\n",
    "            result = f\"_{result}\"\n",
    "            # Load labels vector from the labels file.\n",
    "            self.labels_vector = np.load(dir + '/labels_vector' + result + '.npy')\n",
    "        elif args.use_case == \"Regression\":\n",
    "            self.coords = np.load(dir + \"/coords.npy\")[:, 1:]\n",
    "            self.dist_max = self.dist_max_calc()\n",
    "            \n",
    "        # Sort files based on the sample id which is the number right after \"sample_\". For example, \"sample_5.npy\" will be split into ['sample', '5', '.npy'].\n",
    "        self.file_names.sort(key=lambda x: int(x.split('_')[1].split('.')[0]))\n",
    "            \n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: Number of samples.\n",
    "        \"\"\"\n",
    "        return len(self.file_names)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Construct the full file path for the sample.\n",
    "        file_path = os.path.join(self.dir, self.file_names[idx])\n",
    "        \n",
    "        if args.data_type == 'npy':\n",
    "            # Load the numpy array with shape (n x m x 2).\n",
    "            data = np.load(file_path)\n",
    "        \n",
    "            # Convert the numpy array to a PyTorch tensor.\n",
    "            data = torch.tensor(data, dtype=torch.float32).to(device)\n",
    "        elif args.data_type == 'ISAR':\n",
    "            data = Image.open(file_path).convert(\"RGB\")\n",
    "\n",
    "            # Aplicar transformaciones si las hay\n",
    "            if self.transform:\n",
    "                data = self.transform(data).to(device)\n",
    "            else:\n",
    "                data.to(device)\n",
    "\n",
    "        if args.use_case == \"Classification\":\n",
    "            # Load labels vector from the labels file.\n",
    "            target = torch.tensor(self.labels_vector[idx], dtype=torch.float32).to(device)\n",
    "        elif args.use_case == \"Regression\":\n",
    "            target = torch.tensor(self.coords[idx,:], dtype=torch.float32).to(device)\n",
    "        \n",
    "        return data, target\n",
    "    \n",
    "    def dist_max_calc(self):\n",
    "        sample_coords = self.coords[1].reshape(int(self.coords.shape[1]/3),3)\n",
    "        dist_max = 0.0\n",
    "        for i in range(len(sample_coords)):\n",
    "            if sum(sample_coords[i] ** 2) > dist_max:\n",
    "                dist_max = sum(sample_coords[i] ** 2)\n",
    "            dist_max = np.sqrt(dist_max)\n",
    "        return dist_max\n",
    "\n",
    "class CNNModule(nn.Module):\n",
    "    \"\"\"Convolutional module to extract features from a 2-channel input.\n",
    "\n",
    "    This network processes an input tensor with shape (batch_size, 2, height, width)\n",
    "    and returns a flattened feature vector.\n",
    "    \"\"\"\n",
    "    # La red LSTM podria usarse para relacionar columnas o filas del npy entre sí\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CNNModule, self).__init__()\n",
    "        \n",
    "        # Capas convolucionales\n",
    "        if args.data_type == 'npy':\n",
    "            self.conv1 = nn.LazyConv2d(96, kernel_size=3, stride=2, padding=1)\n",
    "            self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "            self.conv2 = nn.LazyConv2d(256, kernel_size=5, padding=2)\n",
    "            self.pool2 = nn.MaxPool2d(kernel_size=(3, 1), stride=(2, 1), padding=(1, 0))\n",
    "            self.conv3 = nn.LazyConv2d(384, kernel_size=3, padding=1)\n",
    "            self.conv4 = nn.LazyConv2d(384, kernel_size=3, padding=1)\n",
    "            self.conv5 = nn.LazyConv2d(256, kernel_size=3, padding=1)\n",
    "            self.pool3 = nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1))\n",
    "            self.ReLU = nn.ReLU()\n",
    "            self.Flatten = nn.Flatten()\n",
    "\n",
    "        elif args.data_type == 'ISAR':\n",
    "            self.conv1 = nn.LazyConv2d(out_channels=96, kernel_size=22, stride=2, padding=1)\n",
    "            self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "            self.conv2 = nn.LazyConv2d(256, kernel_size=5, padding=2)\n",
    "            self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "            self.conv3 = nn.LazyConv2d(384, kernel_size=3, padding=1)\n",
    "            self.conv4 = nn.LazyConv2d(384, kernel_size=3, padding=1)\n",
    "            self.conv5 = nn.LazyConv2d(256, kernel_size=3, padding=1)\n",
    "            self.pool3 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "            self.ReLU = nn.ReLU()\n",
    "            self.Flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.Flatten(x)\n",
    "        return x\n",
    "    \n",
    "class GeometryPredictor(nn.Module):\n",
    "\n",
    "    def __init__(self, num_labels=None, coords_width=None):\n",
    "\n",
    "        super(GeometryPredictor, self).__init__()\n",
    "        \n",
    "        self.cnn = CNNModule()\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.LazyLinear(4096)  # Adjust size if needed.\n",
    "        self.fc2 = nn.LazyLinear(4096)\n",
    "        \n",
    "        if args.use_case == \"Classification\":\n",
    "            self.fc3 = nn.LazyLinear(num_labels)  # Number of outputs equal to num_labels.\n",
    "        if args.use_case == \"Regression\":\n",
    "            self.fc3 = nn.LazyLinear(coords_width)  # 3*N salidas, una por coordenada (x, y, z) de cada uno de los N vertices\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Process the input through the CNN to extract features.\n",
    "        features = self.cnn(x)\n",
    "\n",
    "        # Pass the features through fully connected layers.\n",
    "        x = F.relu(self.fc1(features))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        output = self.fc3(x)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir las transformaciones (por ejemplo, redimensionar las imágenes y normalizarlas)\n",
    "transform_I = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),                                      # Cambiar según el tamaño de las imágenes\n",
    "    transforms.ToTensor(),                                              # Convertir las imágenes a tensores\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])     # Normalización\n",
    "])\n",
    "\n",
    "# Crear el dataset personalizado\n",
    "dataset = CustomDataset(dir = \"/home/newfasant2/N101/N101-IA/Datasets/Reorganized/Classification_500_0_64_f_64_d\", transform_ISAR=transform_I)\n",
    "\n",
    "if args.use_case == \"Classification\":\n",
    "    # Inicializar modelo\n",
    "    model = GeometryPredictor(num_labels=len(np.unique(dataset.labels_vector))).to(device)\n",
    "\n",
    "# Dividir el dataset en entrenamiento y validación\n",
    "train_size = int(0.7 * len(dataset))  # 70% para entrenamiento\n",
    "val_size = int(0.15 * len(dataset))  # 15% para validación\n",
    "test_size = len(dataset) - train_size - val_size  # 15% para test\n",
    "\n",
    "generator = torch.Generator().manual_seed(101)\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size], generator=generator)\n",
    "\n",
    "# Crear DataLoaders para iterar sobre los datasets\n",
    "train_batch_size = 32\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18785/2180313209.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"/home/newfasant2/N101/N101-IA/CNN/Models/Classification_ISAR_500samples_250ep_32bs.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GeometryPredictor(\n",
       "  (cnn): CNNModule(\n",
       "    (conv1): LazyConv2d(0, 96, kernel_size=(22, 22), stride=(2, 2), padding=(1, 1))\n",
       "    (pool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv2): LazyConv2d(0, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (pool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv3): LazyConv2d(0, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv4): LazyConv2d(0, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv5): LazyConv2d(0, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (pool3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (ReLU): ReLU()\n",
       "    (Flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (fc1): LazyLinear(in_features=0, out_features=4096, bias=True)\n",
       "  (fc2): LazyLinear(in_features=0, out_features=4096, bias=True)\n",
       "  (fc3): LazyLinear(in_features=0, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar un modelo\n",
    "model.load_state_dict(torch.load(\"/home/newfasant2/N101/N101-IA/CNN/Models/Classification_ISAR_500samples_250ep_32bs.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "choice: 36\n",
      "target:  tensor(1., device='cuda:0')\n",
      "output_logits:  tensor([ 1.6384, -1.6513], device='cuda:0')\n",
      "predicted_probs:  tensor([0.9641, 0.0359], device='cuda:0')\n",
      "Loss_torch:  tensor(3.3263, device='cuda:0')\n",
      "Loss own:  tensor(3.3263)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18785/1483665854.py:39: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  loss_own = -np.log((np.exp(logits[label]))/(sum(np.exp(logits))))\n"
     ]
    }
   ],
   "source": [
    "# %matplotlib widget\n",
    "# Importante incluir esta linea al comienzo de la celda donde se quiere visualizar el resultado\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False)\n",
    "\n",
    "random_ch = np.random.randint(0, len(train_dataset))\n",
    "# random_ch = 111\n",
    "print(\"choice:\", random_ch)\n",
    "\n",
    "criterion_CEL = nn.CrossEntropyLoss()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # for npy, target_label in test_loader:\n",
    "            \n",
    "    #     output_logits = model(npy)\n",
    "    #     pred_probs = F.softmax(output_logits, dim=1)\n",
    "    #     loss = criterion_CEL(output_logits, target_label.long())\n",
    "        \n",
    "    #     test_loss = loss.item()\n",
    "    # for npy, target_label in val_loader:\n",
    "            \n",
    "    #     output_logits = model(npy)\n",
    "    #     pred_probs = F.softmax(output_logits, dim=1)\n",
    "    #     loss = criterion_CEL(output_logits, target_label.long())\n",
    "        \n",
    "    #     test_loss = loss.item()\n",
    "    # print(\"test_loss\", test_loss)\n",
    "    # train_results = []\n",
    "    # for data, target_label in train_loader:\n",
    "    #     output_logits = model(data)\n",
    "    #     pred_probs = F.softmax(output_logits, dim=1)\n",
    "    print(\"target: \", target_label[random_ch])\n",
    "    print(\"output_logits: \", output_logits[random_ch])\n",
    "    print(\"predicted_probs: \", pred_probs[random_ch])\n",
    "    print(\"Loss_torch: \", criterion_CEL(output_logits[random_ch], target_label[random_ch].long()))\n",
    "    def cel_own(logits, label):\n",
    "        loss_own = -np.log((np.exp(logits[label]))/(sum(np.exp(logits))))\n",
    "        return loss_own\n",
    "    print(\"Loss own: \", cel_own(output_logits[random_ch].cpu(), target_label[random_ch].long().cpu()))\n",
    "    # for i in range(len(target_label)):\n",
    "    #     train_results.append((int(target_label[i].item()), torch.argmax(pred_probs[i]).item()))\n",
    "    # train_success = train_results[train_results[0]!=train_results[1]]\n",
    "    # train_perc = (1- train_success/len(train_results))*100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    train_results = []\n",
    "    for data, target_label in train_loader:\n",
    "        output_logits = model(data)\n",
    "        pred_probs = F.softmax(output_logits, dim=1)\n",
    "        # print(\"target: \", target_label)\n",
    "        # print(\"predicted: \", pred_probs)\n",
    "    for i in range(len(target_label)):\n",
    "        train_results.append((int(target_label[i].item()), torch.argmax(pred_probs[i]).item()))\n",
    "    train_success = train_results[train_results[0]!=train_results[1]]\n",
    "    train_perc = (1 - len(train_success)/len(train_results))*100\n",
    "\n",
    "    test_results = []\n",
    "    for data, target_label in test_loader:\n",
    "        output_logits = model(data)\n",
    "        pred_probs = F.softmax(output_logits, dim=1)\n",
    "        # print(\"target: \", target_label)\n",
    "        # print(\"predicted: \", pred_probs)\n",
    "    for i in range(len(target_label)):\n",
    "        test_results.append((int(target_label[i].item()), torch.argmax(pred_probs[i]).item()))\n",
    "    test_success = test_results[test_results[0]!=test_results[1]]\n",
    "    test_perc = (1 - len(test_success)/len(test_results))*100\n",
    "\n",
    "    val_results = []\n",
    "    for data, target_label in test_loader:\n",
    "        output_logits = model(data)\n",
    "        pred_probs = F.softmax(output_logits, dim=1)\n",
    "        # print(\"target: \", target_label)\n",
    "        # print(\"predicted: \", pred_probs)\n",
    "    for i in range(len(target_label)):\n",
    "        val_results.append((int(target_label[i].item()), torch.argmax(pred_probs[i]).item()))\n",
    "    val_success = val_results[val_results[0]!=val_results[1]]\n",
    "    val_perc = (1 - len(val_success)/len(val_results))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train percentage of success:  99.42857142857143 %\n",
      "Test percentage of success:  97.33333333333334 %\n",
      "Validation percentage of success:  97.33333333333334 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Train percentage of success: \", train_perc, \"%\")\n",
    "print(\"Test percentage of success: \", test_perc, \"%\")\n",
    "print(\"Validation percentage of success: \", val_perc, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1)\n",
      "(0, 1)\n",
      "(1, 0)\n",
      "(1, 0)\n",
      "(1, 0)\n",
      "(1, 0)\n",
      "(1, 0)\n",
      "(1, 0)\n",
      "(1, 0)\n",
      "(1, 0)\n",
      "(1, 0)\n",
      "(1, 0)\n",
      "(1, 0)\n",
      "(0, 1)\n",
      "(1, 0)\n",
      "test_success: 0.9942857142857143\n"
     ]
    }
   ],
   "source": [
    "target_label.shape\n",
    "\n",
    "results = []\n",
    "\n",
    "for i in range(len(target_label)):\n",
    "    results.append((int(target_label[i].item()), torch.argmax(pred_probs[i]).item()))\n",
    "\n",
    "for pair in results:\n",
    "    if pair[0] != pair[1]:\n",
    "        print(pair)\n",
    "\n",
    "test_success = len(results[results[0]!=results[1]])\n",
    "print(\"test_success:\", 1- test_success/len(results))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "a = np.load(\"/home/newfasant2/N101/N101-IA/Datasets/Raw/Samples_64_f_64_d/Avenger-716_UAV_50000_fixed_meters/sample_116_85.857452_P_isar.npy\")\n",
    "print(a.shape)\n",
    "\n",
    "# plt.imshow(a, cmap=\"gray\")\n",
    "# plt.xticks([])\n",
    "# plt.yticks([])\n",
    "\n",
    "plt.imsave(\"/home/newfasant2/N101/N101-IA/Datasets/avenger.png\", arr=a, cmap=\"gray\", format=\"png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "N101_ev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
